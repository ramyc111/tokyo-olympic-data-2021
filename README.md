# tokyo-olympic-azure-data-engineering-project

This project explores the implementation of end-to-end ETL (Extract, Transform,
Load) operations using Microsoft Azure, a leading cloud platform offering a vast
range of services for data storage, processing, and analytics. Azure provides a
scalable, secure, and flexible ecosystem that supports diverse data engineering tasks,
enabling seamless integration and operational efficiency. The primary objective of
this project was to gain hands-on experience with Azure services, specifically Azure
Data Factory, Azure Databricks, Azure Data Lake Storage Gen2 (ADLS Gen2), and
Azure Synapse Analytics, while integrating these services to build a
functional data pipeline.

Key Highlights:
✅ Data Ingestion: Used Azure Data Factory to pull data from GitHub into Azure Data Lake Storage Gen2 (ADLS Gen2).
✅ Data Transformation: Processed and cleaned data in Azure Databricks with PySpark.
✅ Data Warehousing: Structured transformed data into tables using Azure Synapse Analytics.
✅ Visualization (In Progress): Initiated Power BI integration for actionable insights.

This project was a learning exercise to explore Azure's capabilities, focusing on scalability, integration, and flexibility. It demonstrates the potential of Azure for real-world ETL operations while laying the foundation for production-ready pipelines.
